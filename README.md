# HumanEval with LangChain

ðŸ“– Related article: [Evaluating code generation agentsâ€”LangChain and CodeChain](https://medium.com/@jamesmurdza/evaluating-llms-on-code-generation-langchain-and-codechain-5a804cb1e31c)

This is a demonstration of how to run HumanEval on GPT-3.5 and GPT-4 while taking advantage of LangSmith's visibility and tracing features:

<a href="https://colab.research.google.com/github/jamesmurdza/humaneval-langchain/blob/main/HumanEval_with_LangChain.ipynb" target="_blank">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />
</a>

## Related repositories

- [human-eval](https://github.com/jamesmurdza/humaneval-results): Fork of OpenAI's HumanEval framework used in this workflow.
- [humaneval-results](https://github.com/jamesmurdza/humaneval-results): Repository of HumanEval solutions generated with this workflow.
- [agenteval](https://github.com/jamesmurdza/agenteval): Early version of a framework for evaluating code generation agents.
