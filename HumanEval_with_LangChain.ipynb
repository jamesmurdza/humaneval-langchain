{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U71JKaPsdviX"
      },
      "outputs": [],
      "source": [
        "# This notebook is a demonstration of how to run HumanEval while taking advantage of LangSmith's visibility and tracing features.\n",
        "\n",
        "# To use it:\n",
        "# 1. Update the settings and API keys below.\n",
        "# 2. Run the notebook.\n",
        "# 3. View results in LangSmith."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZHBsa06HEwY"
      },
      "outputs": [],
      "source": [
        "# Dependencies\n",
        "\n",
        "!pip install -q langchain langsmith codechain openai human-eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmwlqXLqepFV"
      },
      "outputs": [],
      "source": [
        "# API keys\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "os.environ[\"ANYSCALE_API_KEY\"] = \"esecret_XXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "\n",
        "# Dataset settings\n",
        "\n",
        "description = \"HumanEval dataset\"\n",
        "\n",
        "dataset_name, max_problems = \"humaneval-small\", 3\n",
        "#dataset_name, max_problems = \"humaneval-all\", False\n",
        "\n",
        "repetitions_per_problem = 5\n",
        "\n",
        "# LLM settings\n",
        "\n",
        "provider, model_name = \"openai\", \"gpt-4\"\n",
        "#provider, model_name = \"anyscale\", \"codellama/CodeLlama-34b-Instruct-hf\"\n",
        "temperature = 0.2\n",
        "\n",
        "# Langsmith settings\n",
        "\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXlkZgIzqVU_"
      },
      "outputs": [],
      "source": [
        "# LangSmith client\n",
        "\n",
        "import langsmith\n",
        "\n",
        "client = langsmith.Client()\n",
        "client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKGxHpHwqtZ4"
      },
      "outputs": [],
      "source": [
        "# Dataset creation\n",
        "\n",
        "from human_eval.data import read_problems\n",
        "\n",
        "# Get the HumanEval dataset up to max_problems.\n",
        "problems = read_problems()\n",
        "if max_problems:\n",
        "  problems = {key: problems[key] for key in list(problems.keys())[:max_problems]}\n",
        "\n",
        "# If the dataset is new, update it to the LangSmith server.\n",
        "if dataset_name not in set([dataset.name for dataset in client.list_datasets()]):\n",
        "  dataset = client.create_dataset(dataset_name, description=description)\n",
        "  for key, value in problems.items():\n",
        "      client.create_example(\n",
        "          inputs={\n",
        "              \"prompt\": value[\"prompt\"],\n",
        "              \"task_id\": key\n",
        "              },\n",
        "          outputs={\n",
        "              \"canonical_solution\": value[\"canonical_solution\"],\n",
        "              },\n",
        "          dataset_id=dataset.id\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cda2zGs3qcDM"
      },
      "outputs": [],
      "source": [
        "# Generation and evaluation\n",
        "\n",
        "from codechain.generation import HumanEvalChain, CompleteCodeChain\n",
        "from codechain.evaluation import HumanEvalEvaluator\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI, ChatAnyscale\n",
        "from langchain.smith import arun_on_dataset, RunEvalConfig\n",
        "\n",
        "import datetime\n",
        "\n",
        "# Factory for the generation chain\n",
        "def chain_factory():\n",
        "    \"\"\"Create a code generation chain.\"\"\"\n",
        "\n",
        "    llm_args = {\n",
        "        \"model_name\": model_name,\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "    if provider == \"openai\":\n",
        "      llm = ChatOpenAI(**llm_args)\n",
        "    elif provider == \"anyscale\":\n",
        "      llm = ChatAnyscale(**llm_args)\n",
        "\n",
        "    return HumanEvalChain.from_llm(llm)\n",
        "\n",
        "# Evaluator configuration\n",
        "evaluation = RunEvalConfig(\n",
        "    custom_evaluators=[HumanEvalEvaluator()],\n",
        "    input_key=\"task_id\"\n",
        "    )\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Run all generations and evaluations\n",
        "for index in range(repetitions_per_problem):\n",
        "\n",
        "  chain_results = await arun_on_dataset(\n",
        "      client=client,\n",
        "      dataset_name=dataset_name,\n",
        "      project_name=f\"HumanEval {timestamp} â€” {index}\",\n",
        "      concurrency_level=10,\n",
        "      llm_or_chain_factory=chain_factory,\n",
        "      evaluation=evaluation,\n",
        "      tags=[\"HumanEval\"],\n",
        "      verbose=True\n",
        "  )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}