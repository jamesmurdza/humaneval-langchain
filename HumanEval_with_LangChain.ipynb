{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U71JKaPsdviX"
      },
      "outputs": [],
      "source": [
        "# This notebook is a demonstration of how to run HumanEval while taking advantage of LangSmith's visibility and tracing features.\n",
        "\n",
        "# To use it:\n",
        "# 1. Update the settings and API keys below\n",
        "# 2. Run the notebook.\n",
        "# 3. View results in LangSmith.\n",
        "\n",
        "# Dependencies:\n",
        "!pip install -q langchain langsmith openai human-eval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Settings\n",
        "\n",
        "#dataset_name, description, max_problems = \"humaneval-all\", \"HumanEval dataset\", False\n",
        "dataset_name, description, max_problems = \"humaneval-small\", \"HumanEval dataset\", 3\n",
        "\n",
        "model_name, temperature = \"gpt-4\", 0.2\n",
        "#model_name, temperature = \"gpt-3.5-turbo\", 0.2\n",
        "\n",
        "repetitions_per_problem = 5"
      ],
      "metadata": {
        "id": "lmwlqXLqepFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Fng3rzPebZP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# API keys:\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
        "\n",
        "# LangSmith settings:\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x8kkN6IjZUn"
      },
      "outputs": [],
      "source": [
        "from human_eval.data import read_problems\n",
        "\n",
        "# Get all HumanEval problems\n",
        "problems = read_problems()\n",
        "\n",
        "# To use a subset of the data during testing\n",
        "if max_problems:\n",
        "  problems = {key: problems[key] for key in list(problems.keys())[:max_problems]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXlkZgIzqVU_"
      },
      "outputs": [],
      "source": [
        "import langsmith\n",
        "\n",
        "# Initialize LangChain+ client\n",
        "client = langsmith.Client()\n",
        "client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKGxHpHwqtZ4"
      },
      "outputs": [],
      "source": [
        "# Create a dataset from the HumanEval problems and solutions\n",
        "\n",
        "if dataset_name not in set([dataset.name for dataset in client.list_datasets()]):\n",
        "\n",
        "  dataset = client.create_dataset(dataset_name, description=description)\n",
        "\n",
        "  for key, value in problems.items():\n",
        "      client.create_example(\n",
        "          inputs={\n",
        "              \"prompt\": value[\"prompt\"],\n",
        "              \"task_id\": key\n",
        "              },\n",
        "          outputs={\n",
        "              \"canonical_solution\": value[\"canonical_solution\"],\n",
        "              },\n",
        "          dataset_id=dataset.id\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3HU5T4PP8Zk"
      },
      "outputs": [],
      "source": [
        "# I would like to abstract everything in this code block into a separate library.\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain, TransformChain, SequentialChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import re\n",
        "\n",
        "# This is a subclass of TransformChain to support async calls.\n",
        "# This is needed by arun_on_dataset to run multiple chains at once.\n",
        "class ATransformChain(TransformChain):\n",
        "      async def _acall(\n",
        "        self,\n",
        "        inputs,\n",
        "        run_manager\n",
        "    ):\n",
        "        return self._call(inputs, run_manager)\n",
        "\n",
        "# This prompt template has been tested with GPT-3.5 and GPT-4.\n",
        "template = \"\"\"\n",
        "```\n",
        "{prompt}\n",
        "```\n",
        "The above is an incomplete Python code fragment. Return the complete and correct code with no additional text.\n",
        "\"\"\"\n",
        "\n",
        "# Define a chain factory producing chains that generate code from chat models\n",
        "\n",
        "def CodeGenerator(model_name, temperature=0.2):\n",
        "\n",
        "  def chain_factory():\n",
        "\n",
        "    # This chain prompts the chat model to generate code.\n",
        "    prompt = PromptTemplate(template=template, input_variables=[\"prompt\"])\n",
        "    llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
        "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "    # This function removes non-code text from a chat response.\n",
        "    def extract_code(inputs: dict) -> dict:\n",
        "      text = inputs[\"text\"]\n",
        "      result = re.search(r'```.*?\\n(.*?)\\n```', text, re.DOTALL)\n",
        "      result = result.group(1) if result else text\n",
        "      return {\"solution\": result}\n",
        "\n",
        "    transform_chain = ATransformChain(\n",
        "        input_variables=[\"text\"],\n",
        "        output_variables=[\"solution\"],\n",
        "        transform=extract_code\n",
        "    )\n",
        "\n",
        "    return SequentialChain(\n",
        "        input_variables=[\"prompt\", \"task_id\"],\n",
        "        output_variables=[\"solution\"],\n",
        "        chains=[llm_chain, transform_chain],\n",
        "    )\n",
        "\n",
        "  return chain_factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDPraOx4slPi"
      },
      "outputs": [],
      "source": [
        "# I would like to abstract everything in this code block into a separate library.\n",
        "# Note that this block is not safeâ€”it runs arbitrary Python code through check_correctness.\n",
        "\n",
        "# Define a Python code evaluator for HumanEval\n",
        "\n",
        "from typing import Optional\n",
        "from langsmith.evaluation import RunEvaluator, EvaluationResult\n",
        "from langsmith.schemas import Run, Example\n",
        "from human_eval.execution import check_correctness\n",
        "\n",
        "class PythonEvaluator(RunEvaluator):\n",
        "    def evaluate_run(self, run: Run, example: Optional[Example] = None) -> EvaluationResult:\n",
        "        print(\"Evaluating \" + example.inputs[\"task_id\"])\n",
        "        problem = problems[example.inputs[\"task_id\"]]\n",
        "        solution = run.outputs[\"solution\"]\n",
        "\n",
        "        # The HumanEval evaluator, which runs the Python code against unit tests\n",
        "        result = check_correctness(problem, solution, 5)\n",
        "\n",
        "        return EvaluationResult(\n",
        "            key = \"Correctness\",\n",
        "            score = bool(result[\"passed\"])\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cda2zGs3qcDM"
      },
      "outputs": [],
      "source": [
        "# Run the generation and evaluation\n",
        "\n",
        "from langchain.smith import arun_on_dataset, RunEvalConfig\n",
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"HumanEval Test - \" + uuid4().hex[0:8]\n",
        "\n",
        "# Run all generations and evaluations\n",
        "chain_results = await arun_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=CodeGenerator(model_name, temperature),\n",
        "    num_repetitions=repetitions_per_problem,\n",
        "    concurrency_level=5,\n",
        "    verbose=True,\n",
        "    client=client,\n",
        "    tags=[\"HumanEval\"],\n",
        "    evaluation=RunEvalConfig(\n",
        "        custom_evaluators=[PythonEvaluator()],\n",
        "        input_key=\"prompt\"\n",
        "        )\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}